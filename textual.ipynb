{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import lib to clear the news\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, TimeDistributed,concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TXT_DATA_FILE = 'data2/news/output_GOOGL.csv'\n",
    "NUM_DATA_FILE = 'data2/news/stockPrices_GOOGL.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_df = pd.read_csv(TXT_DATA_FILE)\n",
    "txt_df['date'] = pd.to_datetime(txt_df['date'])\n",
    "txt_df.sort_values('date',inplace=True)\n",
    "txt_df = txt_df[txt_df['date'] < pd.Timestamp(2019,2,1)]\n",
    "txt_df = txt_df[txt_df['date'] >= pd.Timestamp(2016,1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_df.shape)\n",
    "txt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of news by date\n",
    "# in order to check the dense of news\n",
    "news_num_date = txt_df.groupby(txt_df['date']).count()\n",
    "attribute =  'company'\n",
    "plt.bar(news_num_date.index,news_num_date[attribute])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('number')\n",
    "plt.show()\n",
    "del news_num_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear news \n",
    "# remove non-word and lemmatize words\n",
    "def _clean_text(text):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    text=str(text)\n",
    "    text=re.sub('[^a-zA-Z\\-\\']', ' ',text)  # How to deal with 'NUMBER'?\n",
    "    #text=[lemma.lemmatize(w) for w in word_tokenize(text)] \n",
    "    text=[lemma.lemmatize(w) for w in text.lower().split()]  # 词性还原\n",
    "    text=' '.join(text)\n",
    "    return text\n",
    "\n",
    "def clean_news(df):\n",
    "    text = df['text']\n",
    "    text = _clean_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_df['text'] = txt_df.apply(clean_news, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
