{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import lib to clear the news\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, TimeDistributed, concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TXT_DATA_FILE = 'data2/news/output_GOOGL.csv'\n",
    "NUM_DATA_FILE = 'data2/news/stockPrices_GOOGL.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 10000\n",
    "MAX_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_df = pd.read_csv(TXT_DATA_FILE)\n",
    "txt_df['date'] = pd.to_datetime(txt_df['date'])\n",
    "txt_df.sort_values('date',inplace=True)\n",
    "txt_df = txt_df[txt_df['date'] < pd.Timestamp(2019,2,1)]\n",
    "txt_df = txt_df[txt_df['date'] >= pd.Timestamp(2016,1,1)]\n",
    "txt_df = txt_df.drop(['company'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_df.shape)\n",
    "txt_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of news by date\n",
    "# in order to check the dense of news\n",
    "news_num_date = txt_df.groupby(txt_df['date']).count()\n",
    "attribute =  'text'\n",
    "plt.bar(news_num_date.index,news_num_date[attribute])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('number')\n",
    "plt.show()\n",
    "del news_num_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear news \n",
    "# remove non-word and lemmatize words\n",
    "def _clean_text(text):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    text=str(text)\n",
    "    text=re.sub('[^a-zA-Z\\-\\']', ' ',text)  # How to deal with 'NUMBER'?\n",
    "    #text=[lemma.lemmatize(w) for w in word_tokenize(text)] \n",
    "    text=[lemma.lemmatize(w) for w in text.lower().split()]  # 词性还原\n",
    "    text=' '.join(text)\n",
    "    return text\n",
    "\n",
    "def clean_news(df):\n",
    "    text = df['text']\n",
    "    text = _clean_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_df['text'] = txt_df.apply(clean_news, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the dataframe into dict\n",
    "# dict: pd.Timestamp->news_group\n",
    "def df_to_dict(df):\n",
    "    news_group_dict = dict()\n",
    "    for index, row in df.iterrows():\n",
    "        if row['date'] not in news_group_dict:\n",
    "            news_group_dict[row['date']] = list()\n",
    "        news_group_dict[row['date']].append(row['text'])\n",
    "    return news_group_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide data in to three groups\n",
    "txt_test = df_to_dict(txt_df[txt_df['date'] >= pd.Timestamp(2019,1,1)]) # test_set\n",
    "tmp = txt_df[txt_df['date'] < pd.Timestamp(2019,1,1)]\n",
    "txt_dev = df_to_dict(tmp[tmp['date'] >= pd.Timestamp(2018,9,1)]) # development_set\n",
    "txt_train = df_to_dict(tmp[tmp['date'] < pd.Timestamp(2018,9,1)]) # train_set\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change text into sequences with Keras\n",
    "tmp = txt_df[txt_df['date'] < pd.Timestamp(2019,1,1)]\n",
    "tk_train = tmp[tmp['date'] < pd.Timestamp(2018,9,1)]\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(tk_train['text']))\n",
    "del tmp,tk_train\n",
    "\n",
    "def _text_to_sequences(alist):\n",
    "    tokens = tokenizer.texts_to_sequences(alist)\n",
    "    seqs = pad_sequences(tokens,maxlen=MAX_LEN,truncating='post')\n",
    "    return seqs\n",
    "\n",
    "def text_to_sequences_by_day(adict):\n",
    "    # inplace\n",
    "    for (date,text_list) in adict.items():\n",
    "        adict[date] = _text_to_sequences(text_list)\n",
    "    return adict\n",
    "\n",
    "# overwrite\n",
    "txt_dev = text_to_sequences_by_day(txt_dev)\n",
    "txt_test = text_to_sequences_by_day(txt_test)\n",
    "text_train = text_to_sequences_by_day(txt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import DateOffset\n",
    "def get_x_seqs_by_sw(data_dict, days=3):\n",
    "    # with sliding_window\n",
    "    range_dict = dict()\n",
    "    for (date,news_list) in data_dict.items():\n",
    "        for n_date in pd.date_range(start=date+DateOffset(days=1), periods=days):\n",
    "            if n_date not in range_dict:\n",
    "                range_dict[n_date] = list()\n",
    "            range_dict[n_date].append(news_list)\n",
    "    for key in range_dict:\n",
    "        range_dict[key] = np.array(range_dict[key])\n",
    "    return range_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
