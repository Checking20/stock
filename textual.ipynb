{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import lib to clear the news\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, TimeDistributed, concatenate\n",
    "from keras.models import Model,Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TXT_DATA_FILE = 'data2/news/output_GOOGL.csv'\n",
    "NUM_DATA_FILE = 'data2/prices/stockPrices_GOOGL.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the number of words taken into consideration\n",
    "MAX_FEATURES = 20000\n",
    "# max lenght of one pieces of news\n",
    "MAX_LEN = 30\n",
    "# max number of news taken into consideration per day\n",
    "MAX_NEWS_NUM = 30\n",
    "# days taken into consideration\n",
    "DATE_INTERVAL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_df = pd.read_csv(TXT_DATA_FILE)\n",
    "txt_df['date'] = pd.to_datetime(txt_df['date'])\n",
    "txt_df.sort_values('date',inplace=True)\n",
    "txt_df = txt_df[txt_df['date'] < pd.Timestamp(2019,3,1)]\n",
    "txt_df = txt_df[txt_df['date'] >= pd.Timestamp(2016,1,1)]\n",
    "txt_df = txt_df.drop(['company'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_df = pd.read_csv(NUM_DATA_FILE)\n",
    "num_df['Date'] = pd.to_datetime(num_df['Date'])\n",
    "num_df.sort_values('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide data in to three groups: test development train\n",
    "num_test = num_df[num_df['Date'] >= pd.Timestamp(2019,1,1)].values # test_set\n",
    "tmp = num_df[num_df['Date'] < pd.Timestamp(2019,1,1)]\n",
    "num_dev = tmp[tmp['Date'] >= pd.Timestamp(2018,9,1)].values # development_set\n",
    "num_train = tmp[tmp['Date'] < pd.Timestamp(2018,9,1)].values # train_set\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_df.shape)\n",
    "txt_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of news by date\n",
    "# in order to check the dense of news\n",
    "news_num_date = txt_df.groupby(txt_df['date']).count()\n",
    "attribute =  'text'\n",
    "plt.bar(news_num_date.index,news_num_date[attribute])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('number')\n",
    "plt.show()\n",
    "del news_num_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear news \n",
    "# remove non-word and lemmatize words\n",
    "def _clean_text(text):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    text=str(text)\n",
    "    #text=re.sub('[^a-zA-Z\\-\\']', ' ',text)  # How to deal with 'NUMBER'?\n",
    "    #text=[lemma.lemmatize(w) for w in word_tokenize(text)]\n",
    "    text.replace('\\'s','') #!\n",
    "    text.replace('\\'','') #!\n",
    "    text=[lemma.lemmatize(w) for w in text.lower().split()]  # 词性还原\n",
    "    text=' '.join(text)\n",
    "    text=re.sub('[^a-zA-Z]', ' ' ,text) #!\n",
    "    return text\n",
    "\n",
    "def clean_news(df):\n",
    "    text = df['text']\n",
    "    text = _clean_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_df['text'] = txt_df.apply(clean_news, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the dataframe into dict\n",
    "# map: pd.Timestamp->news_group\n",
    "def df_to_dict(df):\n",
    "    news_group_dict = dict()\n",
    "    for index, row in df.iterrows():\n",
    "        if row['date'] not in news_group_dict:\n",
    "            news_group_dict[row['date']] = list()\n",
    "        news_group_dict[row['date']].append(row['text'])\n",
    "    \n",
    "    for key in news_group_dict:\n",
    "        blank = MAX_NEWS_NUM - len(news_group_dict[key])\n",
    "        if blank >= 0:\n",
    "            # need some blank\n",
    "            for _ in range(blank):\n",
    "                news_group_dict[key].append('')\n",
    "        else:\n",
    "            # need delete some elements\n",
    "            for _ in range(-blank):\n",
    "                # best is 'random'\n",
    "                news_group_dict[key].pop()    \n",
    "    return news_group_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide data in to three groups: test development train\n",
    "txt_test = df_to_dict(txt_df[txt_df['date'] >= pd.Timestamp(2019,1,1)]) # test_set\n",
    "tmp = txt_df[txt_df['date'] < pd.Timestamp(2019,1,1)]\n",
    "txt_dev = df_to_dict(tmp[tmp['date'] >= pd.Timestamp(2018,9,1)]) # development_set\n",
    "txt_train = df_to_dict(tmp[tmp['date'] < pd.Timestamp(2018,9,1)]) # train_set\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change text into sequences with Keras\n",
    "tmp = txt_df[txt_df['date'] < pd.Timestamp(2019,1,1)]\n",
    "tk_train = tmp[tmp['date'] < pd.Timestamp(2018,9,1)]\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(tk_train['text']))\n",
    "del tmp,tk_train\n",
    "\n",
    "def _text_to_sequences(alist):\n",
    "    tokens = tokenizer.texts_to_sequences(alist)\n",
    "    seqs = pad_sequences(tokens,maxlen=MAX_LEN,truncating='post')\n",
    "    return seqs\n",
    "\n",
    "def text_to_sequences_by_day(adict):\n",
    "    # inplace\n",
    "    for (date,text_list) in adict.items():\n",
    "        adict[date] = _text_to_sequences(text_list)\n",
    "    return adict\n",
    "\n",
    "# overwrite\n",
    "txt_dev = text_to_sequences_by_day(txt_dev)\n",
    "txt_test = text_to_sequences_by_day(txt_test)\n",
    "text_train = text_to_sequences_by_day(txt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import DateOffset\n",
    "# generate structed input with sliding_window\n",
    "def get_x_seqs_by_sw(data_dict, days=DATE_INTERVAL):\n",
    "    range_dict = dict()\n",
    "    '''\n",
    "    for (date,news_list) in data_dict.items():\n",
    "        for n_date in pd.date_range(start=date+DateOffset(days=1), periods=days):\n",
    "            if n_date not in range_dict:\n",
    "                range_dict[n_date] = list()\n",
    "            range_dict[n_date].append(news_list)\n",
    "    for key in range_dict:\n",
    "        range_dict[key] = np.array(range_dict[key])\n",
    "    '''\n",
    "    mindate = pd.Timestamp(2099,12,31)\n",
    "    maxdate = pd.Timestamp(2000,1,1)\n",
    "    for date in data_dict.keys():\n",
    "        mindate = min(mindate, date)\n",
    "        maxdate = max(maxdate, date)\n",
    "    maxdate = maxdate+DateOffset(days=1)\n",
    "    mindate = mindate+DateOffset(days=days)\n",
    "    for c_date in pd.date_range(start=mindate, end=maxdate):\n",
    "        range_dict[c_date] = list()\n",
    "        for p_date in pd.date_range(end=c_date-DateOffset(days=days), periods=days):\n",
    "            if p_date in data_dict:\n",
    "                range_dict[c_date].append(np.array(data_dict[p_date]))\n",
    "            else:\n",
    "                range_dict[c_date].append(np.zeros((MAX_NEWS_NUM,MAX_LEN)))\n",
    "        range_dict[c_date] = np.array(range_dict[c_date])\n",
    "    return range_dict\n",
    "\n",
    "# ATTENTION:same as other in 'Numerical'\n",
    "def get_y(data_set):\n",
    "    data_dict =dict()\n",
    "    len9 = len(data_set)\n",
    "    for i in range(len9):\n",
    "        if i > 0:\n",
    "            rate = data_set[i][1]/data_set[i-1][1]-1\n",
    "            if rate <=0:\n",
    "                data_dict[data_set[i][0]] = [1,0]\n",
    "            else:\n",
    "                data_dict[data_set[i][0]] = [0,1]\n",
    "    return data_dict\n",
    "\n",
    "# ATTENTION: same as other in 'Numerical'\n",
    "def match_xy(x_dict,y_dict):\n",
    "    x_list = list()\n",
    "    y_list = list()\n",
    "    for key in x_dict.keys():\n",
    "        if key in y_dict:\n",
    "            x_list.append(x_dict[key])\n",
    "            y_list.append(y_dict[key])\n",
    "    x_arr = np.array(x_list)\n",
    "    y_arr = np.array(y_list)\n",
    "    return (x_arr,y_arr)\n",
    "\n",
    "# match x(news) with y\n",
    "def get_xy_txt(news_data, num_data):\n",
    "    x_dict = get_x_seqs_by_sw(news_data)\n",
    "    y_dict = get_y(num_data)\n",
    "    return match_xy(x_dict,y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train,y_train) = get_xy_txt(txt_train,num_train)\n",
    "(x_test,y_test) = get_xy_txt(txt_test,num_test)\n",
    "(x_dev,y_dev) = get_xy_txt(txt_dev,num_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# the baseline of word embedding\n",
    "in the baseline, we just use the trained word vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use GloVe\n",
    "EMB_FILE = \"tool/GloVe/glove.42B.300d.txt\"\n",
    "#！can't use 840B\n",
    "def get_coefs(word,*arr):\n",
    "    return word,np.asarray(arr,dtype='float32')\n",
    "emb_index = dict(get_coefs(*o.strip().split()) for o in open(EMB_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size=300\n",
    "all_embs = np.stack(emb_index.values())\n",
    "emb_mean = all_embs.mean()\n",
    "emb_std = all_embs.std()\n",
    "word_index = tokenizer.word_index\n",
    "hit_rate = 0\n",
    "ft_words = min(MAX_FEATURES,len(word_index))\n",
    "emb_matrix = np.random.normal(emb_mean,emb_std,(ft_words+1,emb_size))\n",
    "emb_matrix[0] = np.zeros(emb_size)  #！\n",
    "for word, i in word_index.items():\n",
    "    if i > ft_words:\n",
    "        continue\n",
    "    emb_vector = emb_index.get(word)\n",
    "    if emb_vector is not None:\n",
    "        hit_rate += 1\n",
    "        emb_matrix[i] = emb_vector\n",
    "hit_rate = hit_rate/ft_words\n",
    "print('Hit Rate is: ', hit_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model = Sequential()\n",
    "emb_model.add(Embedding(ft_words+1, emb_size, weights=[emb_matrix],trainable=False,\\\n",
    "                        input_shape = (,MAX_NEWS_NUM,MAX_LEN,)))\n",
    "emb_model.compile('rmsprop', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dev = emb_model.predict(x_dev)\n",
    "x_test = emb_model.predict(x_test)\n",
    "x_train = emb_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the baseline of news embedding\n",
    "in the baseline, we just add all word vectors in every news up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_embedding_baseline(x_data):\n",
    "    shape = x_data.shape\n",
    "    shape_dim = len(shape)\n",
    "    return np.mean(x_data,axis=shape_dim-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = news_embedding_baseline(x_test)\n",
    "x_train = news_embedding_baseline(x_train)\n",
    "x_dev = news_embedding_baseline(x_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
