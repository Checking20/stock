{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, TimeDistributed, concatenate\n",
    "from keras.models import Model,Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TXT_DATA_FILE = 'data2/news/news_GOOGL.csv'\n",
    "NUM_DATA_FILE = 'data2/prices/stockPrices_GOOGL.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the number of words taken into consideration\n",
    "MAX_FEATURES = 20000\n",
    "# max lenght of one pieces of news\n",
    "MAX_LEN = 30\n",
    "# max number of news taken into consideration per day\n",
    "MAX_NEWS_NUM = 30\n",
    "# !!! same as one in data_util\n",
    "DATE_INTERVAL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txt_df = pd.read_csv(TXT_DATA_FILE)\n",
    "txt_df['date'] = pd.to_datetime(txt_df['date'])\n",
    "txt_df.sort_values('date',inplace=True)\n",
    "txt_df = txt_df[txt_df['date'] < pd.Timestamp(2019,3,1)]\n",
    "txt_df = txt_df[txt_df['date'] >= pd.Timestamp(2016,1,1)]\n",
    "txt_df = txt_df.drop(['company'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_df = pd.read_csv(NUM_DATA_FILE)\n",
    "num_df['Date'] = pd.to_datetime(num_df['Date'])\n",
    "num_df.sort_values('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide data in to three groups: test development train\n",
    "num_test = num_df[num_df['Date'] >= pd.Timestamp(2019,1,1)].values # test_set\n",
    "tmp = num_df[num_df['Date'] < pd.Timestamp(2019,1,1)]\n",
    "num_dev = tmp[tmp['Date'] >= pd.Timestamp(2018,9,1)].values # development_set\n",
    "num_train = tmp[tmp['Date'] < pd.Timestamp(2018,9,1)].values # train_set\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt_df.shape)\n",
    "txt_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of news by date\n",
    "# in order to check the dense of news\n",
    "news_num_date = txt_df.groupby(txt_df['date']).count()\n",
    "attribute =  'text'\n",
    "plt.bar(news_num_date.index,news_num_date[attribute])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('number')\n",
    "plt.show()\n",
    "del news_num_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clear news \n",
    "\n",
    "# import lib to clear the news\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# remove non-word and lemmatize words\n",
    "def _clean_text(text):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    text=str(text)\n",
    "    #text=re.sub('[^a-zA-Z\\-\\']', ' ',text)  # How to deal with 'NUMBER'?\n",
    "    #text=[lemma.lemmatize(w) for w in word_tokenize(text)]\n",
    "    text.replace('\\'s','') #!\n",
    "    text.replace('\\'','') #!\n",
    "    text=[lemma.lemmatize(w) for w in text.lower().split()]  # 词性还原\n",
    "    text=' '.join(text)\n",
    "    text=re.sub('[^a-zA-Z]', ' ' ,text) #!\n",
    "    return text\n",
    "\n",
    "def clean_news(df):\n",
    "    text = df['text']\n",
    "    text = _clean_text(text)\n",
    "    return text\n",
    "\n",
    "txt_df['text'] = txt_df.apply(clean_news, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change the dataframe into dict\n",
    "# map: pd.Timestamp->news_group\n",
    "def df_to_dict(df):\n",
    "    news_group_dict = dict()\n",
    "    for index, row in df.iterrows():\n",
    "        if row['date'] not in news_group_dict:\n",
    "            news_group_dict[row['date']] = list()\n",
    "        news_group_dict[row['date']].append(row['text'])\n",
    "    \n",
    "    for key in news_group_dict:\n",
    "        blank = MAX_NEWS_NUM - len(news_group_dict[key])\n",
    "        if blank >= 0:\n",
    "            # need some blank\n",
    "            for _ in range(blank):\n",
    "                news_group_dict[key].append('')\n",
    "        else:\n",
    "            # need delete some elements\n",
    "            for _ in range(-blank):\n",
    "                # best is 'random'\n",
    "                news_group_dict[key].pop()    \n",
    "    return news_group_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide data in to three groups: test development train\n",
    "txt_test = df_to_dict(txt_df[txt_df['date'] >= pd.Timestamp(2019,1,1)]) # test_set\n",
    "tmp = txt_df[txt_df['date'] < pd.Timestamp(2019,1,1)] # ---bound---\n",
    "txt_dev = df_to_dict(tmp[tmp['date'] >= pd.Timestamp(2018,9,1)]) # development_set\n",
    "txt_train = df_to_dict(tmp[tmp['date'] < pd.Timestamp(2018,9,1)]) # train_set\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change text into sequences with Keras\n",
    "tmp = txt_df[txt_df['date'] < pd.Timestamp(2019,1,1)]\n",
    "tk_train = tmp[tmp['date'] < pd.Timestamp(2018,9,1)]\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(tk_train['text']))\n",
    "del tmp,tk_train\n",
    "\n",
    "\n",
    "def _text_to_sequences(alist):\n",
    "    tokens = tokenizer.texts_to_sequences(alist)\n",
    "    seqs = pad_sequences(tokens,maxlen=MAX_LEN,truncating='post')\n",
    "    return seqs\n",
    "\n",
    "\n",
    "def text_to_sequences_by_day(adict):\n",
    "    # inplace\n",
    "    for (date,text_list) in adict.items():\n",
    "        adict[date] = _text_to_sequences(text_list)\n",
    "    return adict\n",
    "\n",
    "# overwrite\n",
    "txt_dev = text_to_sequences_by_day(txt_dev)\n",
    "txt_test = text_to_sequences_by_day(txt_test)\n",
    "text_train = text_to_sequences_by_day(txt_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_util import get_xy_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train) = get_xy_txt(txt_train,num_train)\n",
    "(x_test, y_test) = get_xy_txt(txt_test,num_test)\n",
    "(x_dev, y_dev) = get_xy_txt(txt_dev,num_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# the baseline of word embedding\n",
    "in the baseline, we just use the trained word vector matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use GloVe\n",
    "EMB_FILE = \"tool/GloVe/glove.42B.300d.txt\"\n",
    "def get_coefs(word,*arr):\n",
    "    return word,np.asarray(arr,dtype='float32')\n",
    "emb_index = dict(get_coefs(*o.strip().split()) for o in open(EMB_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_size=300\n",
    "all_embs = np.stack(list(emb_index.values()))\n",
    "emb_mean = all_embs.mean()\n",
    "emb_std = all_embs.std()\n",
    "word_index = tokenizer.word_index\n",
    "hit_rate = 0\n",
    "ft_words = min(MAX_FEATURES,len(word_index))\n",
    "emb_matrix = np.random.normal(emb_mean,emb_std,(ft_words+1,emb_size))\n",
    "emb_matrix[0] = np.zeros(emb_size)\n",
    "for word, i in word_index.items():\n",
    "    if i > ft_words:\n",
    "        continue\n",
    "    emb_vector = emb_index.get(word)\n",
    "    if emb_vector is not None:\n",
    "        hit_rate += 1\n",
    "        emb_matrix[i] = emb_vector\n",
    "hit_rate = hit_rate/ft_words\n",
    "print('Hit Rate is: ', hit_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_model = Sequential()\n",
    "emb_model.add(Embedding(ft_words+1, emb_size, weights=[emb_matrix],trainable=False,\\\n",
    "                        input_shape = (DATE_INTERVAL,MAX_NEWS_NUM,MAX_LEN)))\n",
    "emb_model.compile('rmsprop', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dev = emb_model.predict(x_dev)\n",
    "x_test = emb_model.predict(x_test)\n",
    "x_train = emb_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the baseline of news embedding\n",
    "in the baseline, we just add all word vectors in every news up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def news_embedding_baseline(x_data):\n",
    "    shape = x_data.shape\n",
    "    shape_dim = len(shape)\n",
    "    return np.mean(x_data,axis=shape_dim-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = news_embedding_baseline(x_test)\n",
    "x_train = news_embedding_baseline(x_train)\n",
    "x_dev = news_embedding_baseline(x_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the baseline of importance recognition\n",
    "in th baseline, we just use RNN to get the representation of the news corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_baseline_model(code='Default'):\n",
    "    news_input = Input(shape=(DATE_INTERVAL,MAX_NEWS_NUM,emb_size))\n",
    "    day_layer = GRU(50, return_sequences=False)\n",
    "    day_layer = TimeDistributed(day_layer)\n",
    "    inv_layer = GRU(50, return_sequences=False)\n",
    "    x = day_layer(news_input)\n",
    "    x = inv_layer(x)\n",
    "    x = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=news_input,outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=x_train,y=y_train,batch_size=16,epochs=200,verbose=1,validation_data=(x_dev,y_dev),\\\n",
    "          callbacks=[TensorBoard(log_dir='model_log/')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=x_test, y=y_test,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
