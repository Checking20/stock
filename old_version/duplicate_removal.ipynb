{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the file, we try to remove the duplicated news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, TimeDistributed,concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean raw texts \n",
    "def clean_news(news_raws):\n",
    "    lemma=WordNetLemmatizer()\n",
    "    news_rows=[]\n",
    "    for i in range(len(news_raws)):\n",
    "        news=str(news_raws[i])\n",
    "        news=re.sub('[^a-zA-Z]', ' ',news) #remove Non-English\n",
    "        news=[lemma.lemmatize(w) for w in word_tokenize(str(news).lower())]  # lemmatize the word\n",
    "        news=' '.join(news)\n",
    "        news_rows.append(news)\n",
    "    return news_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_FILE = 'data/news_reuters.csv'\n",
    "col_names = ['code','name','date','headline','article','importance']\n",
    "news = pd.read_csv(NEWS_FILE,names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['text'] = clean_news(news['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TF-IDF to vectorize text\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectorizer.fit(news['text'] )\n",
    "word=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the deplicated news\n",
    "def remove_deplicates(news_file,rad=10,threshold = 0.9):\n",
    "    dp_list = set()\n",
    "    tops = news.loc[news['importance']=='topStory']\n",
    "    for index in tops.index:\n",
    "        top = list()\n",
    "        top.append(str(news['text'].loc[index]))\n",
    "        li = list(range(index-rad,index+rad+1))\n",
    "        win = [str(item) for item in news['text'].loc[li]]\n",
    "        top = vectorizer.transform(top).toarray()[0]\n",
    "        win = vectorizer.transform(win).toarray()\n",
    "        for i in range(len(win)):\n",
    "            cur = win[i]\n",
    "            up = np.dot(top,cur)\n",
    "            down = math.sqrt(np.dot(top,top))*math.sqrt(np.dot(cur ,cur))\n",
    "            if down == 0 :\n",
    "                continue\n",
    "            sim = up/down\n",
    "            if sim > threshold and rad!=i:\n",
    "                dp_list.add(index-rad+i)\n",
    "    print('need to drop %d news'%(len(dp_list)))\n",
    "    return dp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_list = remove_deplicates(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news2 = news.drop(list(dp_list))\n",
    "news2['value'] = 0\n",
    "news2['value'].loc[news['importance']=='topStory'] = 1\n",
    "news2 = news2.drop(['name','headline','article','importance'],axis=1)\n",
    "news2 = news2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news2.to_csv('data/cleaned_news.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(news),len(news2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 300\n",
    "max_features = 20000\n",
    "maxlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_by_code = dict()\n",
    "codes =  news['code'].drop_duplicates() # 得到股票代码\n",
    "for code in codes:\n",
    "    news_by_code[code] =news2.loc[news2['code']==code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(news2['text']))\n",
    "print('the number of different words:',len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用GloVe词向量\n",
    "EMB_FILE = \"tool/GloVe/glove.42B.300d.txt\"\n",
    "def get_coefs(word,*arr):\n",
    "    return word,np.asarray(arr,dtype='float32')\n",
    "emb_index = dict(get_coefs(*o.strip().split()) for o in open(EMB_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(emb_index.values())\n",
    "emb_mean = all_embs.mean()\n",
    "emb_std = all_embs.std()\n",
    "word_index = tokenizer.word_index\n",
    "hit_rate = 0\n",
    "ft_words = min(max_features,len(word_index))\n",
    "emb_matrix = np.random.normal(emb_mean,emb_std,(ft_words+1,emb_size))\n",
    "for word, i in word_index.items():\n",
    "    if i > ft_words:\n",
    "        continue\n",
    "    emb_vector = emb_index.get(word)\n",
    "    if emb_vector is not None:\n",
    "        hit_rate += 1\n",
    "        emb_matrix[i] = emb_vector\n",
    "    else:\n",
    "        pass\n",
    "        # print(word)\n",
    "hit_rate = hit_rate/ft_words\n",
    "print(\"the percentage of words in dictionary: \", hit_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(ft_words+1,300,weights=[emb_matrix],trainable=False)\n",
    "article_layer = Bidirectional(GRU(50, return_sequences=True),name='article')\n",
    "\n",
    "def build_model(code='Default'):\n",
    "    inpt = Input(shape = (maxlen,) )\n",
    "    x = embedding_layer(inpt)\n",
    "    x = article_layer (x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(50, activation='tanh',name=code)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1,activation='sigmoid')(x)\n",
    "    model = Model(inputs=inpt,outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bc = dict()\n",
    "for code in codes:\n",
    "    model_bc[code] = build_model(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XY(df):\n",
    "    value_list = df['value'].tolist()\n",
    "    news_list = df['text'].tolist()\n",
    "    news_list = tokenizer.texts_to_sequences(news_list)\n",
    "    news_list = pad_sequences(news_list,maxlen=maxlen,truncating='post')\n",
    "    return (np.array(news_list),np.array(value_list).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bc = dict()\n",
    "Y_train_bc = dict()\n",
    "X_test_bc = dict()\n",
    "Y_test_bc = dict()\n",
    "for code in codes:\n",
    "    X_tmp, Y_tmp = get_XY(news_by_code[code])\n",
    "    X_train_bc[code],X_test_bc[code],Y_train_bc[code],Y_test_bc[code] = train_test_split(X_tmp,Y_tmp,test_size=0.2,random_state = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bc['GOOGL'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns = 50\n",
    "for i in range(turns):\n",
    "    for code in codes:\n",
    "        model_bc[code].fit(X_train_bc[code],Y_train_bc[code],batch_size=16,epochs=1,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code in codes:\n",
    "    print(len(news_by_code[code].loc[news_by_code[code]['value']==1]),len(news_by_code[code].loc[news_by_code[code]['value']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bc['AAPL'].fit(X_train_bc['AAPL'],Y_train_bc['AAPL'],batch_size=16,epochs=20,validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
